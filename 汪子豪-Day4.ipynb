{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8850cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入批量大小：1000\n",
      "单层网络：/n\n",
      "Epoch [1/10], Loss: 0.13173384964466095\n",
      "Epoch [2/10], Loss: 0.09708321839570999\n",
      "Epoch [3/10], Loss: 0.0790652260184288\n",
      "Epoch [4/10], Loss: 0.06418188661336899\n",
      "Epoch [5/10], Loss: 0.051808591932058334\n",
      "Epoch [6/10], Loss: 0.04192375764250755\n",
      "Epoch [7/10], Loss: 0.03434714302420616\n",
      "Epoch [8/10], Loss: 0.028638239949941635\n",
      "Epoch [9/10], Loss: 0.024294991046190262\n",
      "Epoch [10/10], Loss: 0.020915543660521507\n",
      "Accuracy on the test set: 77.6%\n",
      "双层网络：/n\n",
      "Epoch [1/10], Loss: 0.22282391786575317\n",
      "Epoch [2/10], Loss: 0.19752736389636993\n",
      "Epoch [3/10], Loss: 0.18238328397274017\n",
      "Epoch [4/10], Loss: 0.13184240460395813\n",
      "Epoch [5/10], Loss: 0.08392530679702759\n",
      "Epoch [6/10], Loss: 0.04860299825668335\n",
      "Epoch [7/10], Loss: 0.041925475001335144\n",
      "Epoch [8/10], Loss: 0.027425896376371384\n",
      "Epoch [9/10], Loss: 0.012908351607620716\n",
      "Epoch [10/10], Loss: 0.01015567034482956\n",
      "Accuracy on the test set: 80.1%\n",
      "三层网络：/n\n",
      "Epoch [1/10], Loss: 0.2749575674533844\n",
      "Epoch [2/10], Loss: 0.21666288375854492\n",
      "Epoch [3/10], Loss: 0.23776815831661224\n",
      "Epoch [4/10], Loss: 0.2541014552116394\n",
      "Epoch [5/10], Loss: 0.06899350881576538\n",
      "Epoch [6/10], Loss: 0.03016042709350586\n",
      "Epoch [7/10], Loss: 0.011312274262309074\n",
      "Epoch [8/10], Loss: 0.008240863680839539\n",
      "Epoch [9/10], Loss: 0.007639714982360601\n",
      "Epoch [10/10], Loss: 0.0024540091399103403\n",
      "Accuracy on the test set: 79.8%\n",
      "四层网络：/n\n",
      "Epoch [1/10], Loss: 0.6995310187339783\n",
      "Epoch [2/10], Loss: 0.10775644332170486\n",
      "Epoch [3/10], Loss: 0.17710243165493011\n",
      "Epoch [4/10], Loss: 0.5403336882591248\n",
      "Epoch [5/10], Loss: 0.10009301453828812\n",
      "Epoch [6/10], Loss: 0.03881791979074478\n",
      "Epoch [7/10], Loss: 0.03069593943655491\n",
      "Epoch [8/10], Loss: 0.005486312322318554\n",
      "Epoch [9/10], Loss: 0.006017069797962904\n",
      "Epoch [10/10], Loss: 0.00043025295599363744\n",
      "Accuracy on the test set: 76.5%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Day 4 Question a:\n",
    "1.感知机的层数和维数均不是越大越好,没有一个固定的规则来确定感知机的层数和维度。\n",
    "2.层数变化：会影响网络的表示能力以及学习能力，增加层数可以提高网络的学习能力，使其能够处理更复杂的模式和输入特征。\n",
    "  但相应地，如果网络的输入模型本身较简单，特征较少，层数过多可能会导致过拟合，是模型效果下降，也可能导致梯度消失或爆炸\n",
    "3.维度变化：维度是指各层的神经元数量，大小一般<=数据最初输入特征，>=数据最终输出特征\n",
    "  增加感知机的维度同样可以增加网络的学习能力和表达能力。高维度可以提供更多的参数和自由度，使得感知机能够学习到更复杂的特征和模式。\n",
    "  但增加维度也会增加网络的计算复杂度和存储需求，可能需要更长的训练时间和更高的设备性能，过高的维度同样可能导致过拟合\n",
    "4.决定感知机的层数和维度要根据具体的任务需求和数据集来进行权衡。一般来说，如果数据集较简单或者维度较低，较低的层数和维度就足够了；\n",
    "  而对于复杂的数据集和任务，适当地增加层数和维度可以提升感知机的性能。\n",
    "5.以下实验中逐步增加网络层数，从结果中可以看出准确率先升后降，在本题中层数为2较合适\n",
    "\"\"\"\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'  #输出svg矢量图格式\n",
    "\n",
    "class MLP_01(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_01, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class MLP_02(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_02, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MLP_03(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_03, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MLP_04(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_04, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def load_mnist(path, kind='train'):  \n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)\n",
    "\n",
    "def get_FashionMNIST(batch_size_input):\n",
    "    image_train, label_train = load_mnist('D:\\DataSet', kind='train')  #FashionMNIST数据集文件存放在D盘DataSet文件夹中\n",
    "    image_test, label_test = load_mnist('D:\\DataSet', kind='t10k')\n",
    "    \n",
    "    X_train = next(iter(data.DataLoader(image_train, batch_size=batch_size_input)))\n",
    "    X_train = X_train.to(torch.float)\n",
    "    X_train = X_train / 255.0   # X_train =(X_train+0.5)/0.5\n",
    "    y_train = next(iter(data.DataLoader(label_train, batch_size=batch_size_input)))\n",
    "\n",
    "    X_test = next(iter(data.DataLoader(image_test, batch_size=batch_size_input)))\n",
    "    X_test = X_test.to(torch.float)\n",
    "    X_test = X_test / 255.0   # X_test =(X_test+0.5)/0.5\n",
    "    y_test = next(iter(data.DataLoader(label_test, batch_size=batch_size_input)))\n",
    "\n",
    "    train_loader=[X_train,y_train]\n",
    "    test_loader=[X_test,y_test]\n",
    "    return train_loader,test_loader\n",
    "\n",
    "def main_func(batch_size_input,model):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    train_loader,test_loader=get_FashionMNIST(batch_size_input)\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in zip(train_loader[0],train_loader[1]):\n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)        \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = batch_size_input\n",
    "        for images, labels in zip(test_loader[0],test_loader[1]):\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs)\n",
    "            if predicted.item() == labels.item():\n",
    "                correct += 1\n",
    "\n",
    "        print(f'Accuracy on the test set: {100 * correct / total}%')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size_input = int(input(\"请输入批量大小：\"))\n",
    "\n",
    "print(\"单层网络：/n\")\n",
    "model_01 = MLP_01()\n",
    "main_func(batch_size_input,model_01)\n",
    "\n",
    "print(\"双层网络：/n\")\n",
    "model_02 = MLP_02()\n",
    "main_func(batch_size_input,model_02)\n",
    "\n",
    "print(\"三层网络：/n\")\n",
    "model_03 = MLP_03()\n",
    "main_func(batch_size_input,model_03)\n",
    "\n",
    "print(\"四层网络：/n\")\n",
    "model_04 = MLP_04()\n",
    "main_func(batch_size_input,model_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51c6ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络维度784-128-64-62-10：/n\n",
      "Epoch [1/10], Loss: 0.19011160731315613\n",
      "Epoch [2/10], Loss: 0.12320341169834137\n",
      "Epoch [3/10], Loss: 0.09708138555288315\n",
      "Epoch [4/10], Loss: 0.08416499942541122\n",
      "Epoch [5/10], Loss: 0.07708911597728729\n",
      "Epoch [6/10], Loss: 0.07311637699604034\n",
      "Epoch [7/10], Loss: 0.07099033147096634\n",
      "Epoch [8/10], Loss: 0.07005282491445541\n",
      "Epoch [9/10], Loss: 0.0699198916554451\n",
      "Epoch [10/10], Loss: 0.07034587115049362\n",
      "Accuracy on the test set: 79.4%\n",
      "网络维度784-512-256-64-10：/n\n",
      "Epoch [1/10], Loss: 0.03482205048203468\n",
      "Epoch [2/10], Loss: 0.08203566074371338\n",
      "Epoch [3/10], Loss: 0.025936244055628777\n",
      "Epoch [4/10], Loss: 0.03734412044286728\n",
      "Epoch [5/10], Loss: 0.021937096491456032\n",
      "Epoch [6/10], Loss: 0.02395077422261238\n",
      "Epoch [7/10], Loss: 0.013306856155395508\n",
      "Epoch [8/10], Loss: 0.02083335444331169\n",
      "Epoch [9/10], Loss: 0.014951078221201897\n",
      "Epoch [10/10], Loss: 0.012693679891526699\n",
      "Accuracy on the test set: 80.0%\n",
      "网络维度784-512-128-32-10：/n\n",
      "Epoch [1/10], Loss: 1.9411110877990723\n",
      "Epoch [2/10], Loss: 1.5949594974517822\n",
      "Epoch [3/10], Loss: 1.5141650438308716\n",
      "Epoch [4/10], Loss: 1.3786386251449585\n",
      "Epoch [5/10], Loss: 1.0069199800491333\n",
      "Epoch [6/10], Loss: 0.8466925621032715\n",
      "Epoch [7/10], Loss: 0.7657555341720581\n",
      "Epoch [8/10], Loss: 0.7815175652503967\n",
      "Epoch [9/10], Loss: 0.5845691561698914\n",
      "Epoch [10/10], Loss: 0.2193242460489273\n",
      "Accuracy on the test set: 51.4%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"为更好看出各层维度对网络的影响，以下实验网络层数定为四\n",
    "   从实验结果可以看出，网络维度之间的跨度不要太大也不要太小，应该根据层数大小适当调整\n",
    "\"\"\"\n",
    "class MLP_05(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_05, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_06(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_06, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class MLP_07(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_07, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "batch_size_input = 500\n",
    "print(\"网络维度784-128-64-62-10：/n\")\n",
    "model_05 = MLP_05()\n",
    "main_func(batch_size_input,model_01)\n",
    "\n",
    "print(\"网络维度784-512-256-64-10：/n\")\n",
    "model_06 = MLP_06()\n",
    "main_func(batch_size_input,model_02)\n",
    "\n",
    "print(\"网络维度784-512-128-32-10：/n\")\n",
    "model_07 = MLP_07()\n",
    "main_func(batch_size_input,model_07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d0c3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入训练次数，退出循环请输入Q：5\n",
      "Epoch [1/5], Loss: 0.9476597309112549\n",
      "Epoch [2/5], Loss: 0.41452911496162415\n",
      "Epoch [3/5], Loss: 0.19183017313480377\n",
      "Epoch [4/5], Loss: 0.08031967282295227\n",
      "Epoch [5/5], Loss: 0.038602314889431\n",
      "Accuracy on the test set: 71.8%\n",
      "请输入训练次数，退出循环请输入Q：10\n",
      "Epoch [1/10], Loss: 0.028441285714507103\n",
      "Epoch [2/10], Loss: 0.02254311740398407\n",
      "Epoch [3/10], Loss: 0.014861349947750568\n",
      "Epoch [4/10], Loss: 0.011741576716303825\n",
      "Epoch [5/10], Loss: 0.0069843437522649765\n",
      "Epoch [6/10], Loss: 0.004871167708188295\n",
      "Epoch [7/10], Loss: 0.0024530577939003706\n",
      "Epoch [8/10], Loss: 0.002004520269110799\n",
      "Epoch [9/10], Loss: 0.001327943871729076\n",
      "Epoch [10/10], Loss: 0.0010353925172239542\n",
      "Accuracy on the test set: 72.2%\n",
      "请输入训练次数，退出循环请输入Q：15\n",
      "Epoch [1/15], Loss: 0.002173206303268671\n",
      "Epoch [2/15], Loss: 0.005254743155092001\n",
      "Epoch [3/15], Loss: 0.007638887036591768\n",
      "Epoch [4/15], Loss: 0.020374532788991928\n",
      "Epoch [5/15], Loss: 0.015342430211603642\n",
      "Epoch [6/15], Loss: 0.011067785322666168\n",
      "Epoch [7/15], Loss: 0.016211122274398804\n",
      "Epoch [8/15], Loss: 0.03643038496375084\n",
      "Epoch [9/15], Loss: 0.019204746931791306\n",
      "Epoch [10/15], Loss: 0.013926592655479908\n",
      "Epoch [11/15], Loss: 0.12488456815481186\n",
      "Epoch [12/15], Loss: 0.08484705537557602\n",
      "Epoch [13/15], Loss: 0.10635945200920105\n",
      "Epoch [14/15], Loss: 0.08003108203411102\n",
      "Epoch [15/15], Loss: 0.15339413285255432\n",
      "Accuracy on the test set: 78.6%\n",
      "请输入训练次数，退出循环请输入Q：20\n",
      "Epoch [1/20], Loss: 0.10208021104335785\n",
      "Epoch [2/20], Loss: 0.0705147460103035\n",
      "Epoch [3/20], Loss: 0.06534682214260101\n",
      "Epoch [4/20], Loss: 0.05263092368841171\n",
      "Epoch [5/20], Loss: 0.04228321462869644\n",
      "Epoch [6/20], Loss: 0.04226435720920563\n",
      "Epoch [7/20], Loss: 0.03576992452144623\n",
      "Epoch [8/20], Loss: 0.030590618029236794\n",
      "Epoch [9/20], Loss: 0.02723589539527893\n",
      "Epoch [10/20], Loss: 0.023598406463861465\n",
      "Epoch [11/20], Loss: 0.020906904712319374\n",
      "Epoch [12/20], Loss: 0.018177704885601997\n",
      "Epoch [13/20], Loss: 0.016382120549678802\n",
      "Epoch [14/20], Loss: 0.014494601637125015\n",
      "Epoch [15/20], Loss: 0.01296942587941885\n",
      "Epoch [16/20], Loss: 0.011959986761212349\n",
      "Epoch [17/20], Loss: 0.010987376794219017\n",
      "Epoch [18/20], Loss: 0.010081089101731777\n",
      "Epoch [19/20], Loss: 0.009303785860538483\n",
      "Epoch [20/20], Loss: 0.00865293201059103\n",
      "Accuracy on the test set: 79.8%\n",
      "请输入训练次数，退出循环请输入Q：25\n",
      "Epoch [1/25], Loss: 0.008007803000509739\n",
      "Epoch [2/25], Loss: 0.0073748016729950905\n",
      "Epoch [3/25], Loss: 0.006781183648854494\n",
      "Epoch [4/25], Loss: 0.006464403588324785\n",
      "Epoch [5/25], Loss: 0.005827461369335651\n",
      "Epoch [6/25], Loss: 0.005601305980235338\n",
      "Epoch [7/25], Loss: 0.005170426331460476\n",
      "Epoch [8/25], Loss: 0.00488362368196249\n",
      "Epoch [9/25], Loss: 0.004568733740597963\n",
      "Epoch [10/25], Loss: 0.0043256767094135284\n",
      "Epoch [11/25], Loss: 0.0040769800543785095\n",
      "Epoch [12/25], Loss: 0.0038745347410440445\n",
      "Epoch [13/25], Loss: 0.0037041164468973875\n",
      "Epoch [14/25], Loss: 0.0034444546326994896\n",
      "Epoch [15/25], Loss: 0.003310083644464612\n",
      "Epoch [16/25], Loss: 0.003190310439094901\n",
      "Epoch [17/25], Loss: 0.003033918561413884\n",
      "Epoch [18/25], Loss: 0.00288380216807127\n",
      "Epoch [19/25], Loss: 0.0027856139931827784\n",
      "Epoch [20/25], Loss: 0.0027064383029937744\n",
      "Epoch [21/25], Loss: 0.002544263144955039\n",
      "Epoch [22/25], Loss: 0.002447230741381645\n",
      "Epoch [23/25], Loss: 0.0023969272151589394\n",
      "Epoch [24/25], Loss: 0.0023174830712378025\n",
      "Epoch [25/25], Loss: 0.0022226886358112097\n",
      "Accuracy on the test set: 79.8%\n",
      "请输入训练次数，退出循环请输入Q：35\n",
      "Epoch [1/35], Loss: 0.0021496538538485765\n",
      "Epoch [2/35], Loss: 0.0020920787937939167\n",
      "Epoch [3/35], Loss: 0.0020308124367147684\n",
      "Epoch [4/35], Loss: 0.0019462230848148465\n",
      "Epoch [5/35], Loss: 0.0018958942964673042\n",
      "Epoch [6/35], Loss: 0.0018346159486100078\n",
      "Epoch [7/35], Loss: 0.0017782127251848578\n",
      "Epoch [8/35], Loss: 0.001725019421428442\n",
      "Epoch [9/35], Loss: 0.0016896746819838881\n",
      "Epoch [10/35], Loss: 0.001642665360122919\n",
      "Epoch [11/35], Loss: 0.0015893458621576428\n",
      "Epoch [12/35], Loss: 0.0015447123441845179\n",
      "Epoch [13/35], Loss: 0.001513408264145255\n",
      "Epoch [14/35], Loss: 0.00145329674705863\n",
      "Epoch [15/35], Loss: 0.0014396075857803226\n",
      "Epoch [16/35], Loss: 0.001396991079673171\n",
      "Epoch [17/35], Loss: 0.0013629442546516657\n",
      "Epoch [18/35], Loss: 0.001335444045253098\n",
      "Epoch [19/35], Loss: 0.0013053239090368152\n",
      "Epoch [20/35], Loss: 0.001278536394238472\n",
      "Epoch [21/35], Loss: 0.0012436520773917437\n",
      "Epoch [22/35], Loss: 0.0012257928028702736\n",
      "Epoch [23/35], Loss: 0.0011835244949907064\n",
      "Epoch [24/35], Loss: 0.0011699505848810077\n",
      "Epoch [25/35], Loss: 0.0011539950501173735\n",
      "Epoch [26/35], Loss: 0.001134109916165471\n",
      "Epoch [27/35], Loss: 0.001105293515138328\n",
      "Epoch [28/35], Loss: 0.0010876698652282357\n",
      "Epoch [29/35], Loss: 0.0010701649589464068\n",
      "Epoch [30/35], Loss: 0.0010515881003811955\n",
      "Epoch [31/35], Loss: 0.0010286045726388693\n",
      "Epoch [32/35], Loss: 0.0010101459920406342\n",
      "Epoch [33/35], Loss: 0.000998951611109078\n",
      "Epoch [34/35], Loss: 0.000982397934421897\n",
      "Epoch [35/35], Loss: 0.0009607228566892445\n",
      "Accuracy on the test set: 79.6%\n",
      "请输入训练次数，退出循环请输入Q：Q\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question b\n",
    "   以下实验中设训练轮数为自变量，其他条件不变\n",
    "   通过对训练轮数的逐步增加可以看出，本次实验的最佳训练轮数约为：20层左右\n",
    "\"\"\"\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "while True:\n",
    "    num_epochs = input(\"请输入训练次数，退出循环请输入Q：\")\n",
    "    if num_epochs == \"Q\":\n",
    "        break\n",
    "    num_epochs = int(num_epochs)\n",
    "    main_func(batch_size_input,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb1ff8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入批量大小，退出循环请输入Q：64\n",
      "Epoch [1/35], Loss: 0.020091824233531952\n",
      "Epoch [2/35], Loss: 0.01933489553630352\n",
      "Epoch [3/35], Loss: 0.018669946119189262\n",
      "Epoch [4/35], Loss: 0.018077963963150978\n",
      "Epoch [5/35], Loss: 0.01753084547817707\n",
      "Epoch [6/35], Loss: 0.01703898049890995\n",
      "Epoch [7/35], Loss: 0.016570089384913445\n",
      "Epoch [8/35], Loss: 0.01613253355026245\n",
      "Epoch [9/35], Loss: 0.015721073374152184\n",
      "Epoch [10/35], Loss: 0.015331629663705826\n",
      "Epoch [11/35], Loss: 0.014969516545534134\n",
      "Epoch [12/35], Loss: 0.01462324894964695\n",
      "Epoch [13/35], Loss: 0.014295547269284725\n",
      "Epoch [14/35], Loss: 0.01398701686412096\n",
      "Epoch [15/35], Loss: 0.01368897408246994\n",
      "Epoch [16/35], Loss: 0.013401546515524387\n",
      "Epoch [17/35], Loss: 0.013135567307472229\n",
      "Epoch [18/35], Loss: 0.012876107357442379\n",
      "Epoch [19/35], Loss: 0.012627055868506432\n",
      "Epoch [20/35], Loss: 0.01239160168915987\n",
      "Epoch [21/35], Loss: 0.012160449288785458\n",
      "Epoch [22/35], Loss: 0.011945733800530434\n",
      "Epoch [23/35], Loss: 0.011737335473299026\n",
      "Epoch [24/35], Loss: 0.011536316946148872\n",
      "Epoch [25/35], Loss: 0.01134233083575964\n",
      "Epoch [26/35], Loss: 0.011155379004776478\n",
      "Epoch [27/35], Loss: 0.010970869101583958\n",
      "Epoch [28/35], Loss: 0.010796467773616314\n",
      "Epoch [29/35], Loss: 0.01062769629061222\n",
      "Epoch [30/35], Loss: 0.010461846366524696\n",
      "Epoch [31/35], Loss: 0.010303756222128868\n",
      "Epoch [32/35], Loss: 0.010150006972253323\n",
      "Epoch [33/35], Loss: 0.010000718757510185\n",
      "Epoch [34/35], Loss: 0.009856956079602242\n",
      "Epoch [35/35], Loss: 0.009714472107589245\n",
      "Accuracy on the test set: 75.0%\n",
      "请输入批量大小，退出循环请输入Q：128\n",
      "Epoch [1/35], Loss: 0.04104406014084816\n",
      "Epoch [2/35], Loss: 0.02027045376598835\n",
      "Epoch [3/35], Loss: 0.014762569218873978\n",
      "Epoch [4/35], Loss: 0.011952213011682034\n",
      "Epoch [5/35], Loss: 0.01018292922526598\n",
      "Epoch [6/35], Loss: 0.008953305892646313\n",
      "Epoch [7/35], Loss: 0.008025187067687511\n",
      "Epoch [8/35], Loss: 0.007298118434846401\n",
      "Epoch [9/35], Loss: 0.006726597901433706\n",
      "Epoch [10/35], Loss: 0.006220029667019844\n",
      "Epoch [11/35], Loss: 0.005796646233648062\n",
      "Epoch [12/35], Loss: 0.005428454838693142\n",
      "Epoch [13/35], Loss: 0.005108043551445007\n",
      "Epoch [14/35], Loss: 0.00482573127374053\n",
      "Epoch [15/35], Loss: 0.004575141705572605\n",
      "Epoch [16/35], Loss: 0.004348346963524818\n",
      "Epoch [17/35], Loss: 0.004140020813792944\n",
      "Epoch [18/35], Loss: 0.003965729381889105\n",
      "Epoch [19/35], Loss: 0.0038088648580014706\n",
      "Epoch [20/35], Loss: 0.00366337806917727\n",
      "Epoch [21/35], Loss: 0.0035321249160915613\n",
      "Epoch [22/35], Loss: 0.003411784302443266\n",
      "Epoch [23/35], Loss: 0.00329927122220397\n",
      "Epoch [24/35], Loss: 0.0031949449330568314\n",
      "Epoch [25/35], Loss: 0.003097024979069829\n",
      "Epoch [26/35], Loss: 0.003003374207764864\n",
      "Epoch [27/35], Loss: 0.0029158953111618757\n",
      "Epoch [28/35], Loss: 0.002835422521457076\n",
      "Epoch [29/35], Loss: 0.00275957933627069\n",
      "Epoch [30/35], Loss: 0.002686821622774005\n",
      "Epoch [31/35], Loss: 0.0026198846753686666\n",
      "Epoch [32/35], Loss: 0.002557342639192939\n",
      "Epoch [33/35], Loss: 0.002496580593287945\n",
      "Epoch [34/35], Loss: 0.002441046992316842\n",
      "Epoch [35/35], Loss: 0.002387413289397955\n",
      "Accuracy on the test set: 81.25%\n",
      "请输入批量大小，退出循环请输入Q：256\n",
      "Epoch [1/35], Loss: 5.2569914259947836e-05\n",
      "Epoch [2/35], Loss: 4.5060096454108134e-05\n",
      "Epoch [3/35], Loss: 4.279521817807108e-05\n",
      "Epoch [4/35], Loss: 4.136476854910143e-05\n",
      "Epoch [5/35], Loss: 4.029192859889008e-05\n",
      "Epoch [6/35], Loss: 3.9457496313843876e-05\n",
      "Epoch [7/35], Loss: 3.838465272565372e-05\n",
      "Epoch [8/35], Loss: 3.6954195820726454e-05\n",
      "Epoch [9/35], Loss: 3.58813522325363e-05\n",
      "Epoch [10/35], Loss: 3.480850500636734e-05\n",
      "Epoch [11/35], Loss: 3.3854863431770355e-05\n",
      "Epoch [12/35], Loss: 3.302042750874534e-05\n",
      "Epoch [13/35], Loss: 3.2066785934148356e-05\n",
      "Epoch [14/35], Loss: 3.111314072157256e-05\n",
      "Epoch [15/35], Loss: 3.0278701160568744e-05\n",
      "Epoch [16/35], Loss: 2.95634672511369e-05\n",
      "Epoch [17/35], Loss: 2.8729025871143676e-05\n",
      "Epoch [18/35], Loss: 2.8013790142722428e-05\n",
      "Epoch [19/35], Loss: 2.7179348762729205e-05\n",
      "Epoch [20/35], Loss: 2.610649426060263e-05\n",
      "Epoch [21/35], Loss: 2.539125671319198e-05\n",
      "Epoch [22/35], Loss: 2.455681169521995e-05\n",
      "Epoch [23/35], Loss: 2.3841574147809297e-05\n",
      "Epoch [24/35], Loss: 2.3245540432981215e-05\n",
      "Epoch [25/35], Loss: 2.2411095415009186e-05\n",
      "Epoch [26/35], Loss: 2.1815061700181104e-05\n",
      "Epoch [27/35], Loss: 2.1219027985353023e-05\n",
      "Epoch [28/35], Loss: 2.074220174108632e-05\n",
      "Epoch [29/35], Loss: 2.0146166207268834e-05\n",
      "Epoch [30/35], Loss: 1.9550132492440753e-05\n",
      "Epoch [31/35], Loss: 1.9073304429184645e-05\n",
      "Epoch [32/35], Loss: 1.8715683836489916e-05\n",
      "Epoch [33/35], Loss: 1.823885577323381e-05\n",
      "Epoch [34/35], Loss: 1.7762025890988298e-05\n",
      "Epoch [35/35], Loss: 1.7523612768854946e-05\n",
      "Accuracy on the test set: 78.90625%\n",
      "请输入批量大小，退出循环请输入Q：Q\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question c\n",
    "   增加训练轮数而减少过拟合的发生，可以通过增大数据集容量或者减小神经网络层数或维度\n",
    "   以下实验在Question b实验次数为25的基础上，逐步增加数据集容量，可以看出准确率有所上升，说明在一定程度上减少了过拟合的发生\n",
    "\"\"\"\n",
    "num_epochs = 35\n",
    "while True:\n",
    "    batch_size_input = input(\"请输入批量大小，退出循环请输入Q：\")\n",
    "    if batch_size_input == \"Q\":\n",
    "        break\n",
    "    batch_size_input = int(batch_size_input)\n",
    "    main_func(batch_size_input,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f53fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四层网络：/n\n",
      "Epoch [1/35], Loss: 2.184281826019287\n",
      "Epoch [2/35], Loss: 2.163968801498413\n",
      "Epoch [3/35], Loss: 2.129973888397217\n",
      "Epoch [4/35], Loss: 2.0471205711364746\n",
      "Epoch [5/35], Loss: 1.8337873220443726\n",
      "Epoch [6/35], Loss: 1.5935473442077637\n",
      "Epoch [7/35], Loss: 1.5717135667800903\n",
      "Epoch [8/35], Loss: 1.5681190490722656\n",
      "Epoch [9/35], Loss: 1.4821841716766357\n",
      "Epoch [10/35], Loss: 1.3865034580230713\n",
      "Epoch [11/35], Loss: 1.321047067642212\n",
      "Epoch [12/35], Loss: 1.2842506170272827\n",
      "Epoch [13/35], Loss: 1.2176593542099\n",
      "Epoch [14/35], Loss: 1.1150157451629639\n",
      "Epoch [15/35], Loss: 0.987224817276001\n",
      "Epoch [16/35], Loss: 0.9322144985198975\n",
      "Epoch [17/35], Loss: 0.7739132642745972\n",
      "Epoch [18/35], Loss: 0.8028359413146973\n",
      "Epoch [19/35], Loss: 0.7161833047866821\n",
      "Epoch [20/35], Loss: 0.6651313900947571\n",
      "Epoch [21/35], Loss: 0.45327290892601013\n",
      "Epoch [22/35], Loss: 0.305682510137558\n",
      "Epoch [23/35], Loss: 0.23097744584083557\n",
      "Epoch [24/35], Loss: 0.12011606991291046\n",
      "Epoch [25/35], Loss: 0.09048458188772202\n",
      "Epoch [26/35], Loss: 0.0643596351146698\n",
      "Epoch [27/35], Loss: 0.05308196693658829\n",
      "Epoch [28/35], Loss: 0.04036969691514969\n",
      "Epoch [29/35], Loss: 0.03164103254675865\n",
      "Epoch [30/35], Loss: 0.039481185376644135\n",
      "Epoch [31/35], Loss: 0.04529687017202377\n",
      "Epoch [32/35], Loss: 0.028026165440678596\n",
      "Epoch [33/35], Loss: 0.060397058725357056\n",
      "Epoch [34/35], Loss: 0.48558321595191956\n",
      "Epoch [35/35], Loss: 0.0819854736328125\n",
      "Accuracy on the test set: 65.625%\n",
      "三层网络：/n\n",
      "Epoch [1/35], Loss: 2.1157314777374268\n",
      "Epoch [2/35], Loss: 1.8448524475097656\n",
      "Epoch [3/35], Loss: 1.5811020135879517\n",
      "Epoch [4/35], Loss: 1.407656192779541\n",
      "Epoch [5/35], Loss: 1.2448947429656982\n",
      "Epoch [6/35], Loss: 1.1183788776397705\n",
      "Epoch [7/35], Loss: 0.9564340114593506\n",
      "Epoch [8/35], Loss: 0.8113276362419128\n",
      "Epoch [9/35], Loss: 0.6941471099853516\n",
      "Epoch [10/35], Loss: 0.5530605912208557\n",
      "Epoch [11/35], Loss: 0.4447694420814514\n",
      "Epoch [12/35], Loss: 0.3476230800151825\n",
      "Epoch [13/35], Loss: 0.28021472692489624\n",
      "Epoch [14/35], Loss: 0.2123398780822754\n",
      "Epoch [15/35], Loss: 0.16712747514247894\n",
      "Epoch [16/35], Loss: 0.1320248246192932\n",
      "Epoch [17/35], Loss: 0.10206201672554016\n",
      "Epoch [18/35], Loss: 0.08302135765552521\n",
      "Epoch [19/35], Loss: 0.06455198675394058\n",
      "Epoch [20/35], Loss: 0.05523544177412987\n",
      "Epoch [21/35], Loss: 0.05166460573673248\n",
      "Epoch [22/35], Loss: 0.03846731036901474\n",
      "Epoch [23/35], Loss: 0.035423193126916885\n",
      "Epoch [24/35], Loss: 0.02719946950674057\n",
      "Epoch [25/35], Loss: 0.028181826695799828\n",
      "Epoch [26/35], Loss: 0.030778247863054276\n",
      "Epoch [27/35], Loss: 0.034394484013319016\n",
      "Epoch [28/35], Loss: 0.03417365998029709\n",
      "Epoch [29/35], Loss: 0.033448994159698486\n",
      "Epoch [30/35], Loss: 0.0359516404569149\n",
      "Epoch [31/35], Loss: 0.036768265068531036\n",
      "Epoch [32/35], Loss: 0.03584951534867287\n",
      "Epoch [33/35], Loss: 0.03531894087791443\n",
      "Epoch [34/35], Loss: 0.0344197079539299\n",
      "Epoch [35/35], Loss: 0.03343746438622475\n",
      "Accuracy on the test set: 70.3125%\n",
      "双层网络：/n\n",
      "Epoch [1/35], Loss: 1.8981940746307373\n",
      "Epoch [2/35], Loss: 1.5531253814697266\n",
      "Epoch [3/35], Loss: 1.316252589225769\n",
      "Epoch [4/35], Loss: 1.103134036064148\n",
      "Epoch [5/35], Loss: 0.8854894042015076\n",
      "Epoch [6/35], Loss: 0.7232779860496521\n",
      "Epoch [7/35], Loss: 0.5656058192253113\n",
      "Epoch [8/35], Loss: 0.4366418123245239\n",
      "Epoch [9/35], Loss: 0.3297545611858368\n",
      "Epoch [10/35], Loss: 0.2554159164428711\n",
      "Epoch [11/35], Loss: 0.1939239203929901\n",
      "Epoch [12/35], Loss: 0.1497454047203064\n",
      "Epoch [13/35], Loss: 0.1217331811785698\n",
      "Epoch [14/35], Loss: 0.09785564988851547\n",
      "Epoch [15/35], Loss: 0.0802641212940216\n",
      "Epoch [16/35], Loss: 0.0667249783873558\n",
      "Epoch [17/35], Loss: 0.05836961418390274\n",
      "Epoch [18/35], Loss: 0.051359012722969055\n",
      "Epoch [19/35], Loss: 0.04675877466797829\n",
      "Epoch [20/35], Loss: 0.04271393269300461\n",
      "Epoch [21/35], Loss: 0.04106671363115311\n",
      "Epoch [22/35], Loss: 0.03894003853201866\n",
      "Epoch [23/35], Loss: 0.03790690749883652\n",
      "Epoch [24/35], Loss: 0.03723019361495972\n",
      "Epoch [25/35], Loss: 0.03726625815033913\n",
      "Epoch [26/35], Loss: 0.03727257251739502\n",
      "Epoch [27/35], Loss: 0.03720802813768387\n",
      "Epoch [28/35], Loss: 0.03755700960755348\n",
      "Epoch [29/35], Loss: 0.03704870864748955\n",
      "Epoch [30/35], Loss: 0.03724535554647446\n",
      "Epoch [31/35], Loss: 0.036545783281326294\n",
      "Epoch [32/35], Loss: 0.03594876453280449\n",
      "Epoch [33/35], Loss: 0.035296157002449036\n",
      "Epoch [34/35], Loss: 0.034506890922784805\n",
      "Epoch [35/35], Loss: 0.03332505375146866\n",
      "Accuracy on the test set: 74.21875%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"以下实验在Question b实验次数为25的基础上\n",
    "   通过逐步减小神经网络层数，可以看出准确率有所上升，说明在一定程度上减少了过拟合的发生\n",
    "\"\"\"\n",
    "\n",
    "num_epochs = 35\n",
    "batch_size_input = 128\n",
    "print(\"四层网络：/n\")\n",
    "model_04 = MLP_04()\n",
    "main_func(batch_size_input,model_04)\n",
    "\n",
    "print(\"三层网络：/n\")\n",
    "model_03 = MLP_03()\n",
    "main_func(batch_size_input,model_03)\n",
    "\n",
    "print(\"双层网络：/n\")\n",
    "model_02 = MLP_02()\n",
    "main_func(batch_size_input,model_02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a68bce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ReLU函数：/n\n",
      "Epoch [1/20], Loss: 0.5295756459236145\n",
      "Epoch [2/20], Loss: 0.157632976770401\n",
      "Epoch [3/20], Loss: 0.0640597939491272\n",
      "Epoch [4/20], Loss: 0.0363403782248497\n",
      "Epoch [5/20], Loss: 0.024771438911557198\n",
      "Epoch [6/20], Loss: 0.015076849609613419\n",
      "Epoch [7/20], Loss: 0.010905900038778782\n",
      "Epoch [8/20], Loss: 0.00753264594823122\n",
      "Epoch [9/20], Loss: 0.00516259903088212\n",
      "Epoch [10/20], Loss: 0.0031724858563393354\n",
      "Epoch [11/20], Loss: 0.0014078239910304546\n",
      "Epoch [12/20], Loss: 0.0007182164117693901\n",
      "Epoch [13/20], Loss: 0.00039915222441777587\n",
      "Epoch [14/20], Loss: 0.0002585315378382802\n",
      "Epoch [15/20], Loss: 0.00014983485743869096\n",
      "Epoch [16/20], Loss: 0.00010990492592100054\n",
      "Epoch [17/20], Loss: 0.00010179955279454589\n",
      "Epoch [18/20], Loss: 6.8662193370983e-05\n",
      "Epoch [19/20], Loss: 4.708655978902243e-05\n",
      "Epoch [20/20], Loss: 3.361645576660521e-05\n",
      "Accuracy on the test set: 73.046875%\n",
      "sigmoid函数：/n\n",
      "Epoch [1/20], Loss: 1.8782379627227783\n",
      "Epoch [2/20], Loss: 1.0648036003112793\n",
      "Epoch [3/20], Loss: 0.7285182476043701\n",
      "Epoch [4/20], Loss: 0.5381039977073669\n",
      "Epoch [5/20], Loss: 0.4043821692466736\n",
      "Epoch [6/20], Loss: 0.3021046221256256\n",
      "Epoch [7/20], Loss: 0.22304445505142212\n",
      "Epoch [8/20], Loss: 0.16327986121177673\n",
      "Epoch [9/20], Loss: 0.11927283555269241\n",
      "Epoch [10/20], Loss: 0.08746064454317093\n",
      "Epoch [11/20], Loss: 0.0646669790148735\n",
      "Epoch [12/20], Loss: 0.04835700988769531\n",
      "Epoch [13/20], Loss: 0.036642204970121384\n",
      "Epoch [14/20], Loss: 0.028166992589831352\n",
      "Epoch [15/20], Loss: 0.021976398304104805\n",
      "Epoch [16/20], Loss: 0.017403509467840195\n",
      "Epoch [17/20], Loss: 0.013984900899231434\n",
      "Epoch [18/20], Loss: 0.011396664194762707\n",
      "Epoch [19/20], Loss: 0.009412200190126896\n",
      "Epoch [20/20], Loss: 0.007871441543102264\n",
      "Accuracy on the test set: 77.1484375%\n",
      "tanh函数：/n\n",
      "Epoch [1/20], Loss: 0.41408059000968933\n",
      "Epoch [2/20], Loss: 0.16328319907188416\n",
      "Epoch [3/20], Loss: 0.08383344858884811\n",
      "Epoch [4/20], Loss: 0.04948515072464943\n",
      "Epoch [5/20], Loss: 0.030591772869229317\n",
      "Epoch [6/20], Loss: 0.01880659908056259\n",
      "Epoch [7/20], Loss: 0.011439917609095573\n",
      "Epoch [8/20], Loss: 0.006987894885241985\n",
      "Epoch [9/20], Loss: 0.004331848584115505\n",
      "Epoch [10/20], Loss: 0.0027898934204131365\n",
      "Epoch [11/20], Loss: 0.0019149314612150192\n",
      "Epoch [12/20], Loss: 0.0013955625472590327\n",
      "Epoch [13/20], Loss: 0.0010349161457270384\n",
      "Epoch [14/20], Loss: 0.0007526425761170685\n",
      "Epoch [15/20], Loss: 0.000542612629942596\n",
      "Epoch [16/20], Loss: 0.0004067785630468279\n",
      "Epoch [17/20], Loss: 0.00032693761750124395\n",
      "Epoch [18/20], Loss: 0.00028010259848088026\n",
      "Epoch [19/20], Loss: 0.00024816294899210334\n",
      "Epoch [20/20], Loss: 0.00021824838768225163\n",
      "Accuracy on the test set: 71.875%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question d\n",
    "   以下网络分别使用不同的激活函数，可以看到不同的激活函数对本次实验有一定影响\n",
    "   从准确率来看，本次实验最适合的激活函数为sigmoid函数\"\"\"\n",
    "import math\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size_input = 512\n",
    "\n",
    "class MLP_08(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_08, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_09(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_09, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = torch.sigmoid\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_10, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = torch.tanh\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "print(\" ReLU函数：/n\")\n",
    "model_08 = MLP_08()\n",
    "main_func(batch_size_input,model_08)\n",
    "\n",
    "print(\"sigmoid函数：/n\")\n",
    "model_09 = MLP_09()\n",
    "main_func(batch_size_input,model_09)\n",
    "\n",
    "print(\"tanh函数：/n\")\n",
    "model_10 = MLP_10()\n",
    "main_func(batch_size_input,model_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bbcb9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question e\\n   不同的batch大小，可能会对训练过程产生不同的影响\\n   数据方面：使用大批次训练可以在一次运算中处理更多的样本数据（但对设备性能要求高）\\n             使用小批次训练可以提供更多的样本差异性，因为每个批次中的样本可能具有不同的特点。这可以帮助模型更好地泛化到未见样本，并减少过拟合的风险。\\n   优化方面：大批次训练可以减少参数更新的频率，因为每个批次的梯度平均值更稳定。\\n             小批次训练可以提供更频繁的参数更新，可能帮助模型更快地收敛。\\n   '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question e\n",
    "   不同的batch大小，可能会对训练过程产生不同的影响\n",
    "   数据方面：使用大批次训练可以在一次运算中处理更多的样本数据（但对设备性能要求高）\n",
    "             使用小批次训练可以提供更多的样本差异性，因为每个批次中的样本可能具有不同的特点。这可以帮助模型更好地泛化到未见样本，并减少过拟合的风险。\n",
    "   优化方面：大批次训练可以减少参数更新的频率，因为每个批次的梯度平均值更稳定。\n",
    "             小批次训练可以提供更频繁的参数更新，可能帮助模型更快地收敛。\n",
    "   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf358c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
